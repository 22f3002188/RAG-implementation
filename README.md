ğŸ“„ File-Based Multimodal RAG for Use-Case / Test-Case Generation
ğŸ“Œ Overview

This project implements a file-based, multimodal Retrieval-Augmented Generation (RAG) system that generates high-quality, structured use cases or test cases grounded strictly in user-provided documents.

The system ingests multiple file types (text, PDFs, DOCX, images via OCR), retrieves relevant context using hybrid search, applies evidence-based guards, and generates JSON-structured outputs suitable for QA and product documentation workflows.

This project was built as part of an AI Engineer Intern Assignment, with strong emphasis on:

RAG correctness

Hallucination prevention

Modular design

Automation

Code quality

ğŸ¯ Key Capabilities

ğŸ“‚ Multimodal File Ingestion

Text / Markdown

PDF

DOCX

Images (PNG / JPG) via OCR

ğŸ§  Hybrid Retrieval

Semantic search (FAISS + embeddings)

Keyword-based matching

Deduplication

ğŸ§ª Use Case / Test Case Generation

Strict JSON output

Preconditions, steps, expected results

Negative & boundary cases

ğŸ›¡ï¸ Safety & Guards

Context-only generation (no hallucinations)

Minimum evidence threshold

Prompt-injection resistance

ğŸ” Debug & Observability

Retrieved chunk inspection

Ingestion & retrieval logs

âš™ï¸ Fully Automated Pipeline

No manual steps after file upload

ğŸ§± Architecture Overview
User Files (PDF / DOCX / TXT / Images)
        â†“
File Loader + OCR
        â†“
Text Normalization
        â†“
Chunking + Deduplication
        â†“
Vector Store (FAISS)
        â†“
Hybrid Retrieval
        â†“
Evidence Guard
        â†“
LLM Generation (JSON Only)
        â†“
Evaluation & Validation

ğŸ“ Project Structure

RAG-implementation/

â”œâ”€â”€ api.py       
# FastAPI orchestration layer

â”œâ”€â”€ requirements.txt

â”œâ”€â”€ README.md

â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ loader.py  
# Multimodal file loading + OCR

â”‚   â”œâ”€â”€ normalizer.py  
# OCR layout cleanup

â”‚   â””â”€â”€ chunker.py    
# Chunking & deduplication

â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ vector_store.py  
# FAISS vector index
â”‚   â””â”€â”€ retriever.py    
# Hybrid retrieval logic

â”œâ”€â”€ generation/
â”‚   â””â”€â”€ generator.py    
# Guarded LLM generation

â”œâ”€â”€ guards/
â”‚   â””â”€â”€ evidence.py      
# Evidence threshold enforcement

â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ basic_eval.py  
# Output validation hooks

â”œâ”€â”€ utils/
â”‚   â””â”€â”€ llm.py         
# Centralized LLM configuration

â””â”€â”€ ui/
    â””â”€â”€ index.html    
# Lightweight web UI

ğŸš€ Getting Started
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/RAG-implementation.git
cd RAG-implementation

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Environment Variables

UPLOAD YOUR CREDENTIALS IN .env.example file

AZURE_OPENAI_ENDPOINT=your_endpoint

AZURE_OPENAI_API_KEY=your_api_key

AZURE_OPENAI_DEPLOYMENT=your_deployment_name

AZURE_OPENAI_API_VERSION=2024-02-15-preview

                 OR

"RUN IN POWERSHELL THAN RESTART YOUR POWERSHELL"

setx AZURE_OPENAI_API_KEY "PUT YOUR KEY HERE"

setx AZURE_OPENAI_API_VERSION "PUT YOUR VERSION HERE"

setx AZURE_OPENAI_ENDPOINT "PUT YOUR ENDPOINT HERE"

setx AZURE_OPENAI_DEPLOYMENT "PUT YOUR MODEL HERE"              

âš ï¸ The project keeps everything file-based and local. No external databases are required.

4ï¸âƒ£ Run the Application
uvicorn api:app --reload


Open your browser at:

http://localhost:8000

ğŸ§ª Example Query

Input:

Create use cases for user signup


Output (JSON):

{
  "status": "success",
  "use_cases": [
    {
      "title": "User Signup with Valid Credentials",
      "preconditions": ["User is not registered"],
      "steps": ["Enter valid email", "Set password", "Submit form"],
      "expected_result": "Account created successfully",
      "negative_cases": ["Invalid email format", "Weak password"]
    }
  ]
}


If insufficient information is available, the system responds with:

{
  "status": "insufficient_info",
  "missing_information": ["Signup fields not defined"]
}

ğŸ›¡ï¸ Safety & Guardrails

âŒ No feature invention

ğŸ“„ Answers strictly grounded in retrieved chunks

ğŸ”’ Ignores instructions embedded inside documents

ğŸ“‰ Blocks LLM calls if evidence is weak

ğŸ§¹ Deduplicates low-quality chunks

ğŸ“Š Evaluation & Debugging

Retrieve and inspect evidence chunks

Logs for ingestion, retrieval, and generation

Basic output validation to ensure schema correctness

âš™ï¸ Tech Stack

Backend: FastAPI

LLM: Azure OpenAI

Embeddings: OpenAI embeddings

Vector Store: FAISS

OCR: Tesseract (via pytesseract)

Parsing: PyPDF, python-docx

Frontend: HTML + Bootstrap

ğŸ§© Design Principles

Modular & extensible

File-based (no mandatory external DB)

Guard-first RAG design

Production-oriented structure

ğŸš§ Future Improvements

Docker support

Reranking models

Token usage & latency metrics

Source citations in output

URL ingestion support

ğŸ‘¤ Author

Harsh Jayswal
AI Engineer Intern Candidate
